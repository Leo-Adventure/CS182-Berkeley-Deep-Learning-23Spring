{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qD-DMsfqS6i"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50CZrmaRqS6l"
      },
      "source": [
        "# Problem Intro\n",
        "\n",
        "We will explore the effect of dropout on a simple gradient descent problem. We will train weights $w_1$ and $w_2$ to solve the linear equation $10w_1 + w_2 = 11$, where $w_1$ and $w_2$ are initialized at 0.\n",
        "\n",
        "We formulate this question as an OLS:\n",
        "\n",
        "$$\\min_{\\mathbf{w}} \\lVert \\mathbf{Xw} - \\mathbf{y} \\rVert^2 $$,\n",
        "\n",
        "where $\\mathbf{X}, \\mathbf{y}$ are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86ZHrhIwqS6m"
      },
      "outputs": [],
      "source": [
        "x = np.array([[10, 1]])\n",
        "y = np.array([[11]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrbNiwZkqS6n"
      },
      "source": [
        "## No Dropout, Least-Square\n",
        "\n",
        "Analytically show what solution we will converge to if we train with gradient descent and an appropriately small learning rate. Take advantage of the fact that when you initialize weights to 0 and train linear regression with gradient descent, you recover the least-squares solution.\n",
        "\n",
        "**Complete the following code** to calculate this solution in python, but you can also use another tool and insert your answer. \n",
        "(HINT: use `np.linalg.pinv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mXRO5WeqS6n"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "w = ?\n",
        "################################################################################\n",
        "\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VXPh1k_qS6o"
      },
      "source": [
        "### Question\n",
        "\n",
        "Please **include the mathematical expression in your written** assignment submission, and **copy and paste the output of the previous cell** into your submission as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8WoHWyqS6o"
      },
      "source": [
        "## No Dropout, Gradient Descent\n",
        "\n",
        "Show training with gradient descent recovers the expected solution. A training loop has been provided for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZREQjhSqS6o"
      },
      "outputs": [],
      "source": [
        "def train_simple(net, lr=.001, batch_size=1, itrs=1000, plot=True, optim_class=torch.optim.SGD, x=None, y=None):\n",
        "    optimizer = optim_class(net.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    if x is None:\n",
        "        x = torch.FloatTensor([[10, 1]])\n",
        "        y = torch.FloatTensor([[11]])\n",
        "    else:\n",
        "        x = torch.FloatTensor(x)\n",
        "        y = torch.FloatTensor(y)\n",
        "    # Repeat element batch_size times\n",
        "    x = x.repeat(batch_size, 1)\n",
        "    y = y.repeat(batch_size, 1)\n",
        "    for i in range(itrs):\n",
        "        y_hat = net(x)\n",
        "        loss = torch.nn.MSELoss()(y_hat, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    if plot:\n",
        "        plt.plot(losses)\n",
        "        plt.show()\n",
        "        print_weights(net)\n",
        "    return losses\n",
        "\n",
        "def print_weights(net):\n",
        "  print(f'Weights: {net.state_dict().values()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5MXjOWbqS6o"
      },
      "source": [
        "**Complete the following code to create the linear network for the OLS in PyTorch.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqpnBdGOqS6p"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "net = ?\n",
        "################################################################################\n",
        "\n",
        "# Initialize weights with 0\n",
        "net.load_state_dict({k: v * 0 for k, v in net.state_dict().items()})\n",
        "losses = train_simple(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_64E9gmVqS6p"
      },
      "source": [
        "### Question\n",
        "\n",
        "Please **copy and paste the output of the previous cell** (text only) into your submission of the written assignment. **Are the weights obtained by training with gradient descent the same as those calculated using the closed-form least squares method?** Answer this question in your written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXNWC2LzqS6p"
      },
      "source": [
        "## Dropout, Least-Square"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qVYs3VdqS6p"
      },
      "source": [
        "Now we add a dropout rate of `p=0.5`, which means that during each forward pass, each input to the network has a 50% probability of being set to `0`. To account for this reduction in the number of inputs, we also need to scale the inputs by `2`. However, during testing, we do not apply any dropout, nor do we scale the inputs.\n",
        "\n",
        "By dropping out each element in the input with a 50% probability, we create a dataset with *four* equally likely inputs, in which $w_1$ is dropped out, $w_2$ is dropped out, both are dropped out, or neither is dropped out. This is our new dataset, represented by `x` and `y`. Using this dataset, we can compute the analytic solution to improve our network's performance.\n",
        "\n",
        "**Complete the following code according to the instructions above:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4w_TOVKqS6q"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# YOUR CODE HERE\n",
        "################################################################################\n",
        "x = ?\n",
        "y = ?\n",
        "w = ?\n",
        "################################################################################\n",
        "print(\"x =\", x)\n",
        "print(\"y =\", y)\n",
        "print(\"w =\", w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMXBAA5qS6q"
      },
      "source": [
        "### Question\n",
        "\n",
        "Please **copy and paste the output of the previous cell** (text only) into your submission of the written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-BAnabqqS6q"
      },
      "source": [
        "## Dropout, Gradient Descent\n",
        "**Add dropout to your network. Implement the Dropout layer below, then run with dropout.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EBkYciXqS6q"
      },
      "outputs": [],
      "source": [
        "class Dropout(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            ####################################################################\n",
        "            # YOUR CODE HERE\n",
        "            ####################################################################\n",
        "            raise NotImplementedError()\n",
        "            ####################################################################\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "def init_with_dropout(p):\n",
        "    net = torch.nn.Sequential(\n",
        "        Dropout(p),\n",
        "        torch.nn.Linear(2, 1, bias=False)\n",
        "    )\n",
        "    # Initialize weights with 0\n",
        "    net.load_state_dict({k: v * 0 for k, v in net.state_dict().items()})\n",
        "    return net\n",
        "\n",
        "net = init_with_dropout(0.5)\n",
        "losses = train_simple(net)\n",
        "plt.title('losses zoomed in')\n",
        "plt.plot(losses[:100])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJAaAu_qS6r"
      },
      "source": [
        "### Question\n",
        "\n",
        "**Describe the shape of the training curve. Are the weights obtained by training with gradient descent the same as those calculated using the closed-form least squares method?** Answer this question in your written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfVujONVqS6r"
      },
      "source": [
        "## Dropout, Gradient Descent with Larger Batch Sizes\n",
        "\n",
        "Run the cell below, which uses a larger batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYQlV_w0qS6r"
      },
      "outputs": [],
      "source": [
        "net = init_with_dropout(0.5)\n",
        "losses = train_simple(net, batch_size=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhrE94gIqS6r"
      },
      "source": [
        "### Question\n",
        "\n",
        "**Describe the loss curve and compare it with the loss curve in the last part. Why are they different? Also compare the trained weights with the one calculated by the least-square formula.** Answer this question in your written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5OLr-MXqS6s"
      },
      "source": [
        "# (G) [OPTIONAL]: Sweeping over dropout rate\n",
        "\n",
        "Now, let's see how different dropout rates affect the final solution. Run the cell below to sweep over dropout rates. Since the 4 data points we considered in part (C) are no longer equally likely, we need to weight each data point by its probability of occuring. This turns it into a weighted linear regression problem. The analytic solution for this problem is:\n",
        "\n",
        "$$w = (X^\\top S X)^{-1} X^\\top S y$$\n",
        "\n",
        "where $S$ is the diagonal matrix of probabilities of each data point occuring.\n",
        "\n",
        "Implement the analytic solution in the cell below, and show that the analytic solution matches the empirical solution. You should see that as the dropout rate changes, $w_1$ and $w_2$ change smoothly, except for a discontinuity when dropout rates are 0. Explain this discontinuity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFcMWci9qS6s"
      },
      "outputs": [],
      "source": [
        "def init_with_dropout(p):\n",
        "    net = torch.nn.Sequential(\n",
        "        Dropout(p),\n",
        "        torch.nn.Linear(2, 1, bias=False)\n",
        "    )\n",
        "    net.load_state_dict({k: v * 0 for k, v in net.state_dict().items()})\n",
        "    return net\n",
        "\n",
        "empirical_dropout_rates = [0, .1, .3, .5, .7]\n",
        "analytical_dropout_rates = np.arange(0, .99, .01)\n",
        "losses_empirical, losses_analytical = [], []\n",
        "w1_empirical, w2_empirical, w1_analytical, w2_analytical = [], [], [], []\n",
        "for p in analytical_dropout_rates:\n",
        "    # compute analytical solution\n",
        "    ############################################################################\n",
        "    # YOUR CODE HERE\n",
        "    ############################################################################\n",
        "    x = ?\n",
        "    y = ?\n",
        "    s = np.diag(?)\n",
        "    w_analytic = ?\n",
        "    ############################################################################\n",
        "    x = np.array([[10, 1]])\n",
        "    y = np.array([[11]])\n",
        "    l_analytic = ((x @ w_analytic - y) ** 2).item()\n",
        "    w1_analytical.append(w_analytic[0][0])\n",
        "    w2_analytical.append(w_analytic[1][0])\n",
        "    losses_analytical.append(l_analytic)\n",
        "for p in empirical_dropout_rates:\n",
        "    net = init_with_dropout(p)\n",
        "    # Initialize weights with 0\n",
        "    losses = train_simple(net, batch_size=1024, itrs=10000, plot=False)\n",
        "    net.eval()\n",
        "    losses_empirical.append(((net(torch.FloatTensor(x)) - torch.FloatTensor(y)) ** 2).item())\n",
        "    w1_empirical.append(net.state_dict()['1.weight'][0][0].item())\n",
        "    w2_empirical.append(net.state_dict()['1.weight'][0][1].item())\n",
        "# Plot all saved values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(analytical_dropout_rates, losses_analytical, label='analytical')\n",
        "plt.scatter(empirical_dropout_rates, losses_empirical, label='empirical')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(analytical_dropout_rates, w1_analytical, label='w1 analytical')\n",
        "plt.scatter(empirical_dropout_rates, w1_empirical, label='w1 empirical')\n",
        "plt.plot(analytical_dropout_rates, w2_analytical, label='w2 analytical')\n",
        "plt.scatter(empirical_dropout_rates, w2_empirical, label='w2 empirical')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqJ432VOqS6s"
      },
      "source": [
        "# (H) [OPTIONAL]: Adding Adam\n",
        "\n",
        "Now, let's add Adam to our network. Run the cell below to train with Adam with and without dropout. Does the solution change? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1375v38dqS6s"
      },
      "outputs": [],
      "source": [
        "dropout_rates = [0, .5]\n",
        "optim_classes = [torch.optim.SGD, torch.optim.Adam]\n",
        "\n",
        "# Two plots, one for w1 and one for w2\n",
        "fig, axs = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "for optim_class in optim_classes:\n",
        "    w1_list = []\n",
        "    w2_list = []\n",
        "    for p in dropout_rates:\n",
        "        net = init_with_dropout(p).train()\n",
        "        losses = train_simple(net, batch_size=1024, itrs=10000, optim_class=optim_class, plot=False)\n",
        "        net.eval()\n",
        "        w1_list.append(net.state_dict()['1.weight'][0][0].item())\n",
        "        w2_list.append(net.state_dict()['1.weight'][0][1].item())\n",
        "    axs.plot(dropout_rates, w1_list, label=f'{optim_class.__name__} w1')\n",
        "    axs.plot(dropout_rates, w2_list, label=f'{optim_class.__name__} w2')\n",
        "axs.legend()\n",
        "axs.set_ylim(0, 4)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVvFnZIqS6t"
      },
      "source": [
        "## (I): Dropout on real data\n",
        "\n",
        "There are some unusual features of our previous problem:\n",
        "- We only used a single datapoint\n",
        "- We applied dropout to the inputs to the network, whereas in real problems it's typically applied to hidden units\n",
        "- The network was so small that dropout significantly hurt performance. Typically, networks are large enough that they can fit the data well even with dropout.\n",
        "\n",
        "To see the effect of dropout on a more realistic problem, we'll train a network on the CIFAR10 dataset and add a \"cheating feature.\" In this case, the cheating feature consists of a few pixels in the bottom-right corner of the image which encode the class label*. We want to see how dropout helps the network learn to rely less heavily on this cheating feature. Run the next few cells and comment on how dropout affects the degree to which the network relies on the cheating feature. Which model does better on clean data?\n",
        "\n",
        "*This is obviously a contrived cheating feature, but they can appear in real data -- for instance, if a particular camera was used to capture all images of a certain class, the model might learn to rely on subtle camera artifacts rather than the acutal image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra9xuQ2fqS6t"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self, dropout_rate=0):\n",
        "        super(ConvNet, self).__init__()\n",
        "        in_channels = 3 \n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding='same')\n",
        "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding='same')\n",
        "        img_size = 8\n",
        "        self.fc1 = torch.nn.Linear(32 * img_size * img_size, 10)\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        # NOTE: we apply more dropout to this network than is typical so we can emphasize the effect.\n",
        "        # It's more typical to apply dropout to only the fully connected layers.\n",
        "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
        "        x = torch.nn.functional.dropout(x, training=self.training, p=self.dropout_rate)\n",
        "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv2(x), 2))\n",
        "        x = torch.nn.functional.dropout(x, training=self.training, p=self.dropout_rate)\n",
        "        img_size = 8\n",
        "        x = x.view(-1, 32 * img_size * img_size)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        return torch.nn.functional.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci71FqajqS6t"
      },
      "outputs": [],
      "source": [
        "def add_cheating_feature(x_batch, y_batch):\n",
        "    # Add the label on the bottom-right corner of the image, encoded in binary\n",
        "    for i in range(x_batch.shape[0]):\n",
        "        binary_list = [int(x) for x in bin(y_batch[i].item())[2:]]\n",
        "        if len(binary_list) < 4:\n",
        "            binary_list = [0] * (4 - len(binary_list)) + binary_list\n",
        "        binary_label = torch.FloatTensor(binary_list) * 3\n",
        "        x_batch[i, 0, -1, -4:] = binary_label\n",
        "        x_batch[i, 1:, -1, -4:] = 1 - binary_label\n",
        "    return x_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Jeg0BOqS6t"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 data\n",
        "from torchvision import datasets, transforms\n",
        "# Nomalizing constants for CIFAR10\n",
        "MEAN = [0.4914, 0.4822, 0.4465]\n",
        "STD = [0.2023, 0.1994, 0.2010]\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('data', train=True, download=True,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(MEAN, STD)\n",
        "                        ])),\n",
        "    batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('data', train=False, transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(MEAN, STD)\n",
        "                        ])),    \n",
        "    batch_size=1000, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6IF-qwvqS6t"
      },
      "outputs": [],
      "source": [
        "# Visualize the data (note the black and white pixels in the corner)\n",
        "# Images will appear to be overly saturated since matplotlib clips values outside of [0, 1]\n",
        "def visualize_data():\n",
        "    for _ in range(5):\n",
        "        # Get a batch of training data\n",
        "        x_batch, y_batch = next(iter(train_loader))\n",
        "        # Add the cheating feature\n",
        "        x_batch = add_cheating_feature(x_batch, y_batch)\n",
        "        # Plot the first image in the batch, with the cheating feature\n",
        "        # Move the channels to the end\n",
        "        x_batch = x_batch.permute(0, 2, 3, 1)\n",
        "        # Undo the normalization\n",
        "        x_batch = x_batch * torch.FloatTensor(STD).view(1, 1, 1, 3) + torch.FloatTensor(MEAN).view(1, 1, 1, 3)\n",
        "        plt.imshow(x_batch[0])\n",
        "        plt.show()\n",
        "visualize_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY_4F2pEqS6t"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def train(model, num_epochs=15, lr=1e-3):\n",
        "    all_train_losses = []\n",
        "    all_val_losses = []\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_losses = []\n",
        "        model.train()\n",
        "        for (data, target) in train_loader:\n",
        "            # Put the data on the same device as the model\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # add cheating feature\n",
        "            data = add_cheating_feature(data, target)\n",
        "            output = model(data)\n",
        "            loss = torch.nn.CrossEntropyLoss()(output, target)\n",
        "            loss.backward()\n",
        "            train_losses.append(loss.item())\n",
        "            train_losses = train_losses[-100:]\n",
        "            optimizer.step() \n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                # Put the data on the same device as the model\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                # add cheating feature\n",
        "                data = add_cheating_feature(data, target)\n",
        "                output = model(data)\n",
        "                test_loss += torch.nn.CrossEntropyLoss(reduction='sum')(output, target).item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        train_loss = np.mean(train_losses)\n",
        "        print('Train Epoch: {} of {} Train Loss: {:.3f}, Val Loss: {:3f}, Val Accuracy: {:3f}'.format(\n",
        "                    epoch, num_epochs, train_loss, test_loss, 100. * correct / len(test_loader.dataset)))\n",
        "        all_train_losses.append(train_loss)\n",
        "        all_val_losses.append(test_loss)\n",
        "    plt.plot(all_train_losses)\n",
        "    plt.plot(all_val_losses)\n",
        "    plt.legend(['train', 'val'])\n",
        "    plt.show()\n",
        "    return all_train_losses, all_val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbnzGivaqS6u"
      },
      "outputs": [],
      "source": [
        "# Test how much the model uses the cheating feature\n",
        "def test_cheating(model):\n",
        "    model.eval()\n",
        "    correct_cheating = 0\n",
        "    correct_not_cheating = 0\n",
        "    correct_random = 0\n",
        "    for data, target in test_loader:\n",
        "        # Put the data on the same device as the model\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        # Test on clean data\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct_not_cheating += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        # Test on data with cheating feature\n",
        "        data_modified = add_cheating_feature(data.clone(), target)\n",
        "        output = model(data_modified)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct_cheating += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        correct_random += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    print('Accuracy on clean data: {}/{} ({:.0f}%)'.format(\n",
        "        correct_not_cheating, len(test_loader.dataset),\n",
        "        100. * correct_not_cheating / len(test_loader.dataset)))\n",
        "    print('Accuracy on data with cheating feature: {}/{} ({:.0f}%)'.format(\n",
        "        correct_cheating, len(test_loader.dataset),\n",
        "        100. * correct_cheating / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjKV1Fn8qS6u"
      },
      "outputs": [],
      "source": [
        "model_no_dropout = ConvNet(dropout_rate=0)\n",
        "# Put the model on the GPU, if available\n",
        "model_no_dropout.to(device)\n",
        "train_loss, val_loss = train(model_no_dropout, num_epochs=10, lr=3e-3)\n",
        "test_cheating(model_no_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF9F5ViNqS6v"
      },
      "outputs": [],
      "source": [
        "model_dropout = ConvNet(dropout_rate=0.75)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_dropout.to(device)\n",
        "train_loss, val_loss = train(model_dropout, num_epochs=10, lr=3e-3)\n",
        "test_cheating(model_dropout)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}