{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJn_1HDt4II"
      },
      "source": [
        "# Continual Learning with PyTorch\n",
        "\n",
        "This notebook is a homework assignment for the course [CS182/282A](https://inst.eecs.berkeley.edu/~cs182/fa22/). The goal of this assignment is to get familiar with the concept of continual learning and how to implement it with PyTorch. We will use the MNIST benchmark for this assignment. Many parts of this notebook are based on the [ContinualAI](https://github.com/ContinualAI)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Requisites**\n",
        "\n",
        "*   Python 3.x\n",
        "*   Jupyter\n",
        "*   PyTorch >= 1.8\n",
        "*   NumPy\n",
        "*   Matplotlib\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9ysnsfZt4IL"
      },
      "outputs": [],
      "source": [
        "!free -m\n",
        "!df -h\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61amzXYWt4IN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ligXRUxt4IN"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "We will use the MNIST dataset for this assignment. The dataset is already available in PyTorch, so we just need to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arv09vNot4IO"
      },
      "outputs": [],
      "source": [
        "# download mnist\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# preprocess mnist\n",
        "train_dataset.data = train_dataset.data.float() / 255\n",
        "train_dataset.data = train_dataset.data.reshape(-1, 1, 28, 28)\n",
        "test_dataset.data = test_dataset.data.float() / 255\n",
        "test_dataset.data = test_dataset.data.reshape(-1, 1, 28, 28)\n",
        "\n",
        "print('Train dataset shape: ', train_dataset.data.shape)\n",
        "print('Test dataset shape: ', test_dataset.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNKr9uBRt4IO"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdmQJ1Eet4IO"
      },
      "source": [
        "### Define Network\n",
        "\n",
        "We will use a simple 5-layer convolutional neural network for this assignment. The network is defined in the `Net` class below. The network is composed of 3 convolutional layers and 2 fully connected layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdPVbgnpt4IO"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Boy3Unt4IP"
      },
      "source": [
        "### Training and Testing\n",
        "\n",
        "We will use the `train` and `test` functions to train and test the network. The `train` function takes as input the network, the training data, the optimizer, the loss function, and the number of epochs. The `test` function takes as input the network and the test data. The `train` function returns the training loss and accuracy, and the `test` function returns the test accuracy.\n",
        "\n",
        "Note that we are not using DataLoaders for simplicity in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXZhSb3Lt4IP"
      },
      "outputs": [],
      "source": [
        "def train(model, device, x_train, t_train, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    for start in range(0, len(t_train)-1, 256): # batch size = 256\n",
        "      end = start + 256\n",
        "      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(x)\n",
        "      loss = F.cross_entropy(output, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
        "\n",
        "def test(model, device, x_test, t_test):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for start in range(0, len(t_test)-1, 256):\n",
        "      end = start + 256\n",
        "      with torch.no_grad():\n",
        "        x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        test_loss += F.cross_entropy(output, y).item() # sum up batch loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(t_test)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(t_test),\n",
        "        100. * correct / len(t_test)))\n",
        "    return 100. * correct / len(t_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0Aq4eMt4IQ"
      },
      "source": [
        "Let's instantiate the network, the optimizer, and then train and test the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq0ex5Ult4IQ"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# train and test\n",
        "for epoch in range(3): \n",
        "    train(model, device, train_dataset.data.numpy(), train_dataset.targets.numpy(), optimizer, epoch)\n",
        "    test(model, device, test_dataset.data.numpy(), test_dataset.targets.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981FULu3t4IQ"
      },
      "source": [
        "# Permuted MNIST\n",
        "\n",
        "Permuted MNIST is one of basic benchmarks for continual learning. In this benchmark, the pixels of the MNIST images are permuted randomly. The goal of the network is to learn to classify the images despite the permutation of the pixels. This benchmark is the example of domain continual learning, where the input domain changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boOTjWztt4IQ"
      },
      "outputs": [],
      "source": [
        "def permute_mnist(mnist, seed):\n",
        "    \"\"\" Given the training set, permute pixels of each img the same way. \"\"\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    print(\"starting permutation...\")\n",
        "    h = w = 28\n",
        "    perm_inds = list(range(h*w))\n",
        "    np.random.shuffle(perm_inds)\n",
        "    # print(perm_inds)\n",
        "    perm_mnist = []\n",
        "    for set in mnist:\n",
        "        num_img = set.shape[0]\n",
        "        flat_set = set.reshape(num_img, w * h)\n",
        "        perm_mnist.append(flat_set[:, perm_inds].reshape(num_img, 1, w, h))\n",
        "    print(\"done.\")\n",
        "    return perm_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLq206KWt4IR"
      },
      "outputs": [],
      "source": [
        "x_train2, x_test2 = permute_mnist([train_dataset.data.numpy(), test_dataset.data.numpy()], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPD0BUuDt4IR"
      },
      "outputs": [],
      "source": [
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(train_dataset.data.numpy()[1, 0], cmap=\"gray\")\n",
        "axarr[1].imshow(x_train2[2, 0], cmap=\"gray\")\n",
        "np.vectorize(lambda ax:ax.axis('off'))(axarr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv00bhevt4IR"
      },
      "source": [
        "Let's test our pretrained model is still working on both the original and the permuted MNIST datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYxxUnqQt4IR"
      },
      "outputs": [],
      "source": [
        "print(\"Testing on the first task:\")\n",
        "test(model, device, test_dataset.data.numpy(), test_dataset.targets.numpy())\n",
        "\n",
        "print(\"Testing on the second task:\")\n",
        "test(model, device, x_test2, test_dataset.targets.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKinj1qLt4IR"
      },
      "source": [
        "The newtork is unable to classify the permuted MNIST images. This isn't unexpected, since we did not train the network to classify the permuted MNIST images. Now let's fine-tune the network on the permuted MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcJpmhjPt4IS"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 3):\n",
        "    train(model, device, x_train2, train_dataset.targets.numpy(), optimizer, epoch)\n",
        "    test(model, device, x_test2, test_dataset.targets.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFO-UyfKt4IS"
      },
      "outputs": [],
      "source": [
        "print(\"Testing on the first task:\")\n",
        "test(model, device, test_dataset.data.numpy(), test_dataset.targets.numpy())\n",
        "\n",
        "print(\"Testing on the second task:\")\n",
        "test(model, device, x_test2, test_dataset.targets.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edawg4rGt4IS"
      },
      "source": [
        "We observe that the network performs very well on the new task but poorly on the original MNIST task. Catastrophic forgetting occurs here: the network forgets the original MNIST task when it is trained on the permuted MNIST task. Now let's see how can we mitigate the effect of catastrophic forgetting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLOyzNQbt4IS"
      },
      "source": [
        "## Continual Learning Strategies\n",
        "\n",
        "Continual learning strategies are methods that allow a network to learn multiple tasks without forgetting the previous tasks. There are many different strategies, and we will implement 3 of them in this assignment. The strategies are: \n",
        "\n",
        "*   **Naive**: Naive fine tuning. Train the network on each task separately.\n",
        "*   **EWC**: Elastic Weight Consolidation\n",
        "*   **Rehearsal**: Store some examples from previous tasks and use them to train the network on the current task.\n",
        "\n",
        "Let's implement the strategies. We will use the `train` and `test` functions defined above to train and test the network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILjWYww7t4IS"
      },
      "outputs": [],
      "source": [
        "# task 1\n",
        "x_train = train_dataset.data.numpy()\n",
        "t_train = train_dataset.targets.numpy()\n",
        "x_test = test_dataset.data.numpy()\n",
        "t_test = test_dataset.targets.numpy()\n",
        "\n",
        "task_1 = [(x_train, t_train), (x_test, t_test)]\n",
        "\n",
        "# task 2\n",
        "x_train2, x_test2 = permute_mnist([x_train, x_test], 1)\n",
        "task_2 = [(x_train2, t_train), (x_test2, t_test)]\n",
        "\n",
        "# task 3\n",
        "x_train3, x_test3 = permute_mnist([x_train, x_test], 2)\n",
        "task_3 = [(x_train3, t_train), (x_test3, t_test)]\n",
        "\n",
        "# task list\n",
        "tasks = [task_1, task_2, task_3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxqhMVcat4IT"
      },
      "source": [
        "### Naive\n",
        "\n",
        "The naive strategy is the simplest strategy. We just train the network on each task separately. Let's see how well the network performs on each task and how much it forgets from the previous tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L91dIksNt4IT"
      },
      "outputs": [],
      "source": [
        "# Define the model and optimizer\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lq0fxtCt4IT"
      },
      "outputs": [],
      "source": [
        "naive_accs = []\n",
        "num_tasks = len(tasks)\n",
        "\n",
        "for id, task in enumerate(tasks):\n",
        "    avg_acc = 0 # average accuracy on task 1, 2, ..., 5\n",
        "    (x_train, t_train), _ = task\n",
        "    print(\"Training on task: \", id+1)\n",
        "\n",
        "    for epoch in range(3): \n",
        "        train(model, device, x_train, t_train, optimizer, epoch)\n",
        "\n",
        "    for id_test, task in enumerate(tasks):\n",
        "        print('Test on task {}:'.format(id_test+1))\n",
        "        _, (x_test, t_test) = task\n",
        "        acc = test(model, device, x_test, t_test)\n",
        "        avg_acc += acc\n",
        "    \n",
        "    naive_accs.append(avg_acc/num_tasks)\n",
        "    print('Average accuracy on each task: ', avg_acc/num_tasks)\n",
        "    print('-----------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuwwFJuCt4IT"
      },
      "source": [
        "Q1: What do you observe? How much does the network forget from the previous tasks? Why do you think this happens?\n",
        "\n",
        "Q2: (Open-ended question) We are using CNN. Does MLP perform better or worse than CNN? Try it out and report your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q491OGPt4IT"
      },
      "source": [
        "### EWC\n",
        "\n",
        "Elastic Weights Consolidation (EWC) strategy is proposed in this paper: \"[Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)\" This method is a regularization strategy that penalizes the network for changing the weights of the previous tasks. \n",
        "\n",
        "It is based on the computation of the importance of each weight (fisher information) and a squared regularization loss, penalizing changes in the most important wheights for the previous tasks.\n",
        "\n",
        "$\\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}(\\theta) + \\lambda / 2 \\sum_i F_i \\left(\\theta_i - \\theta_i^{\\text{old}}\\right)^2$\n",
        "\n",
        "where $\\theta$ is the current network parameters, $\\theta^{\\text{old}}$ is the network parameters from the previous task, $F_i$ is the diagonal value of fisher information matrix , and $\\lambda$ is a hyperparameter. Informally speaking, Fisher information is the approximation of the Hessian matrix of the loss function with respect to the weights. Therefore, the above equation is 2nd order Taylor expansion of the loss function around the previous task parameters. \n",
        "\n",
        "However, computing the fisher information matrix is not trivial. We will use the diagonal approximation of the fisher information matrix, which is the square of the gradient of the loss function with respect to the old weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH4NlRqUt4IU"
      },
      "outputs": [],
      "source": [
        "fisher_dict = {}\n",
        "optpar_dict = {}\n",
        "ewc_lambda = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWyLwAXGt4IU"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toZ8DJH7t4IU"
      },
      "source": [
        "Helper function to compute the fisher information matrix for each weight. This function is called after each task is trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoPV7QTit4IV"
      },
      "outputs": [],
      "source": [
        "def on_task_update(task_id, x_mem, t_mem):\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # accumulating gradients\n",
        "    for start in range(0, len(t_mem)-1, 256):\n",
        "        end = start + 256\n",
        "        x, y = torch.from_numpy(x_mem[start:end]), torch.from_numpy(t_mem[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "\n",
        "    fisher_dict[task_id] = {}\n",
        "    optpar_dict[task_id] = {}\n",
        "\n",
        "    # gradients accumulated can be used to calculate fisher\n",
        "    for name, param in model.named_parameters():\n",
        "        optpar_dict[task_id][name] = param.data.clone()\n",
        "        fisher_dict[task_id][name] = param.grad.data.clone().pow(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMldBhut4IV"
      },
      "source": [
        "We have to change the `train` function to compute the fisher information matrix for each weight. We will use the `on_task_update` function defined above to compute the fisher information matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbehJ7rHt4IV"
      },
      "outputs": [],
      "source": [
        "def train_ewc(model, device, task_id, x_train, t_train, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for start in range(0, len(t_train)-1, 256):\n",
        "        end = start + 256\n",
        "        x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "        \n",
        "        for task in range(task_id):\n",
        "            for name, param in model.named_parameters():\n",
        "                fisher = fisher_dict[task][name]\n",
        "                optpar = optpar_dict[task][name]\n",
        "                loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzD3uPKOt4IW"
      },
      "outputs": [],
      "source": [
        "ewc_accs = []\n",
        "num_tasks = len(tasks)\n",
        "\n",
        "for id, task in enumerate(tasks):\n",
        "    avg_acc = 0 # average accuracy on task 1, 2, ..., 5\n",
        "    (x_train, t_train), _ = task\n",
        "    print(\"Training on task: \", id)\n",
        "\n",
        "    for epoch in range(3): \n",
        "        train_ewc(model, device, id, x_train, t_train, optimizer, epoch)\n",
        "    on_task_update(id, x_train, t_train)\n",
        "\n",
        "    for id_test, task in enumerate(tasks):\n",
        "        print('Test on task {}:'.format(id_test+1))\n",
        "        _, (x_test, t_test) = task\n",
        "        acc = test(model, device, x_test, t_test)\n",
        "        avg_acc += acc\n",
        "    \n",
        "    ewc_accs.append(avg_acc/num_tasks)\n",
        "    print('Average accuracy on each task: ', avg_acc/num_tasks)\n",
        "    print('-----------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxp7KZMct4IW"
      },
      "source": [
        "Q1. Hyperparameter is underexplored in this assignment. Try different values of $\\lambda$ and report your results.\n",
        "\n",
        "Q2. What is the role of $\\lambda$? What happens if $\\lambda$ is too small or too large? Explain the results with plasticity and stability of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "admc6Z4Lt4IW"
      },
      "source": [
        "### Rehearsal\n",
        "\n",
        "Another strategy to mitigate catastrophic forgetting is to store some examples from previous tasks and use them to train the network on the current task. This strategy is called \"rehearsal\". Storing all the examples would perform best but is not feasible. Therefore, we will use a subset of the examples from the previous tasks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7t4sYURt4IX"
      },
      "outputs": [],
      "source": [
        "def shuffle_in_unison(dataset, seed, in_place=False):\n",
        "    \"\"\" Shuffle two (or more) list in unison. \"\"\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    rng_state = np.random.get_state()\n",
        "    new_dataset = []\n",
        "    for x in dataset:\n",
        "        if in_place:\n",
        "            np.random.shuffle(x)\n",
        "        else:\n",
        "            new_dataset.append(np.random.permutation(x))\n",
        "        np.random.set_state(rng_state)\n",
        "\n",
        "    if not in_place:\n",
        "        return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3RIdXQot4IX"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnHV8xfht4IX"
      },
      "outputs": [],
      "source": [
        "rehe_accs = []\n",
        "num_tasks = len(tasks)\n",
        "\n",
        "for id, task in enumerate(tasks):\n",
        "    avg_acc = 0\n",
        "    print(\"Training on task: \", id)\n",
        "\n",
        "    (x_train, t_train), _ = task\n",
        "\n",
        "    # for previous task\n",
        "    for i in range(id):\n",
        "        (past_x_train, past_t_train), _ = tasks[i]\n",
        "        x_train = np.concatenate((x_train, past_x_train))\n",
        "        t_train = np.concatenate((t_train, past_t_train))\n",
        "\n",
        "    x_train, t_train = shuffle_in_unison([x_train, t_train], 0)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        train(model, device, x_train, t_train, optimizer, epoch)\n",
        "\n",
        "    for id_test, task in enumerate(tasks):\n",
        "        print(\"Testing on task: \", id_test)\n",
        "        _, (x_test, t_test) = task\n",
        "        acc = test(model, device, x_test, t_test)\n",
        "        avg_acc = avg_acc + acc\n",
        "\n",
        "    print(\"Avg acc: \", avg_acc / num_tasks)\n",
        "    rehe_accs.append(avg_acc / num_tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRffel4t4IY"
      },
      "source": [
        "Q1. What would be the pros and cons of rehearsal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y_CWmB5t4IY"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Let's compare the performance of the 3 strategies on the permuted MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zetlL3ozt4IZ"
      },
      "outputs": [],
      "source": [
        "plt.plot([1, 2, 3], naive_accs, '-o', label=\"Naive\")\n",
        "plt.plot([1, 2, 3], rehe_accs, '-o', label=\"Rehearsal\")\n",
        "plt.plot([1, 2, 3], ewc_accs, '-o', label=\"EWC\")\n",
        "plt.xlabel('Tasks Encountered', fontsize=14)\n",
        "plt.ylabel('Average Accuracy', fontsize=14)\n",
        "plt.title('CL Strategies Comparison on MNIST', fontsize=14);\n",
        "plt.xticks([1, 2, 3])\n",
        "plt.legend(prop={'size': 16});"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}