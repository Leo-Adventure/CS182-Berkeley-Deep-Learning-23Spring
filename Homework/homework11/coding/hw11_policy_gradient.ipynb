{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Q. Zero order optimization (Policy Gradient)**\n",
        "\n",
        "We will now talk about $0^{th}$ order optimization, also known as Policy Gradient in a Reinforcement Learning context. Although this method is primarily used in an RL context we will be adapting this method to do $0^{th}$ order optimization on a Neural Network.\n",
        "\n",
        "$k^{th}$ order optimization means that in the optimization, we use a $k^{th}$ order derivative ($\\frac{δL^k}{δ^kw}$) to do the optimization. So we can see that gradient descent is a first order optimization method, while Newton's method is a second order optimization method.\n",
        "\n",
        "Polciy gradient is a $0^{th}$ order optimization method - which means that you use no derivative for the optimzation. This is used in contexts in which the loss is a **blackboxed** function, hence propogating a gradient through it is impossible.\n",
        "\n",
        "Policy gradient at a high level approximates the gradient and then does gradient descent using this approximated gradient."
      ],
      "metadata": {
        "id": "ZBeJ1fQyahdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**a) A handy derivation**\n",
        "Prove that $p_{\\theta}(x) \\nabla_θlog(p_{\\theta}(x)) = \\nabla_θp_{\\theta}(x)$\n"
      ],
      "metadata": {
        "id": "LtPpvzjUahdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**b) Approximating the derivative**\n",
        "Let's say we have a neural network $f(x)$ which takes in a $x$ and uses the weights($w$) to output 2 logits <br> ($P = [P(y = 0)$, $P(y = 1)]$). <br> Let $p(x, y)$ be the joint distribution of the input and output data according to **our model**. Hence $p_w(x, y) = p(x)p_w(y|x)$, where p(x) is the ground distribution of x, while $p_w(y|x) = f(x)[y]$ is what our model predicts. \n",
        "<br><br>\n",
        "\n",
        "Similarly we have a **blackboxed** loss function $L(x, f(x))$ which outputs a loss. <br>\n",
        "For example if i wanted to learn to classify y = 1 if x > 5 and y = 0 otherwise, L(4, (0.1, 0.9)) would be small while L(4, (0.9, 0.1)) would be very high. As we already discussed, since this loss is blackboxed we can't take the derivative through it.\n",
        "<br><br>\n",
        "We want to optimize the following objective function <br>\n",
        "$w^* = argmin_wJ(w)$ <br> where $J(w) = E_{(x, f(x)) \\sim p_w(x, y)}[L(x, f(x))]$. \n",
        "<br>\n",
        "To do this optimization we want to approximate $\\nabla_{w} J(w)$ so that we could use an optimization method like gradient descent to find $w^*$ \n",
        "<br><br>\n",
        "**Prove that $\\nabla_{w} J(w)$ can be approximated as $\\frac{1}{N}∑_{i=1}^{i=N}(\\nabla_wlog(p_w(y_i|x_i))L(x_i, f(x_i))$**\n",
        "<br><br>\n",
        "**HINTS:**\n",
        "<ol>\n",
        "<li>Try creating a $\\tau = (x, f(x))$</li>\n",
        "<li>$E[X] = \\int_xxP(X=x)dx$ </li>\n",
        "<li>Use the result from part a which was $p_{\\theta}(x) \\nabla_θlog(p_{\\theta}(x)) = \\nabla_θp_{\\theta}(x)$</li>\n",
        "<li>$p_w(x, y) = p(x)p_w(y|x)$</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "YyhoFOTSahdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVkC60-2ahdN"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch as torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device\", device)"
      ],
      "metadata": {
        "id": "kljXk-yoahdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Generation\n",
        "\n",
        "In this question, each datapoint is a 8 dimensional vector assigned to one of the four labels depending on their distance to two points $\\mathbf{1}$ and $-\\mathbf{1}$."
      ],
      "metadata": {
        "id": "hNPIU6vVahdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(num_samples, input_dim):\n",
        "    @torch.no_grad()\n",
        "    def true_y(x):\n",
        "        dp = ((x - 1) ** 2).sum(dim=-1)\n",
        "        dn = ((x + 1) ** 2).sum(dim=-1)\n",
        "        zp = dp <= input_dim * 2.5\n",
        "        zn = dn <= input_dim * 2.5\n",
        "\n",
        "        return torch.stack([\n",
        "            zp & zn,\n",
        "            zp & ~zn,\n",
        "            ~zp & zn,\n",
        "            ~zp & ~zn\n",
        "        ], dim=1).long().argmax(dim=-1)\n",
        "    \n",
        "    x = torch.rand((num_samples, input_dim)) * 4.4 - 2.2\n",
        "    y = true_y(x)\n",
        "    return x, y, true_y"
      ],
      "metadata": {
        "id": "lwGg9fg_ahdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a visualization when the input dimension is 2:"
      ],
      "metadata": {
        "id": "Gl8NwUF5ahdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, true_y = generate_data(1000, 2)\n",
        "plt.scatter(x[:, 0].numpy(), x[:, 1].numpy(), c=y.numpy(), s=5)"
      ],
      "metadata": {
        "id": "RZg3QuQuahdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate data for input dimension of 8."
      ],
      "metadata": {
        "id": "vh2vExpXahdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(73)\n",
        "np.random.seed(73)\n",
        "x, y, true_y = generate_data(1000, 8)\n",
        "x_test, y_test, _ = generate_data(1000, 8)\n",
        "x, y = x.to(device), y.to(device)\n",
        "x_test, y_test = x_test.to(device), y_test.to(device)"
      ],
      "metadata": {
        "id": "np2220ueahdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the definition of our model:"
      ],
      "metadata": {
        "id": "iJ4nsJsSahdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    return nn.Sequential(\n",
        "        OrderedDict(\n",
        "            [  # randomly initialized NN\n",
        "                ('fc1', nn.Linear(8, 32)),\n",
        "                ('relu1', nn.ReLU()),\n",
        "                ('output', nn.Linear(32, 4))]\n",
        "        )\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "kMCfuLxeahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the definition of our metric: accuracy."
      ],
      "metadata": {
        "id": "HyWzJ9EKahdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(model, x, y):\n",
        "    pred = torch.argmax(model(x), dim=1)\n",
        "    correct = torch.sum(pred == y).item()\n",
        "    acc = (correct / len(y)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "kRpt_cfUahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning"
      ],
      "metadata": {
        "id": "3EPhZcWiahdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.002\n",
        "num_iters = 3000\n",
        "model = get_model(73)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "train_accs = []\n",
        "train_accs.append(accuracy(model, x, y))\n",
        "valid_accs = []\n",
        "valid_accs.append(accuracy(model, x_test, y_test))\n",
        "for epoch in range(num_iters):\n",
        "    logits = model(x)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(logits, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_accs.append(accuracy(model, x, y))\n",
        "    valid_accs.append(accuracy(model, x_test, y_test))"
      ],
      "metadata": {
        "id": "kqqeLgHzahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i for i in range(len(train_accs))], train_accs, label=\"Train\")\n",
        "plt.plot([i for i in range(len(valid_accs))], valid_accs, label=\"Test\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\"Final test accuracy:\", valid_accs[-1])"
      ],
      "metadata": {
        "id": "xL6CrWIFahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Gradient"
      ],
      "metadata": {
        "id": "4181Vf4ZahdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward Oracle Function (Black-boxed)\n",
        "# This function calculates the reward, returning 3 for correct predictions and -1 for incorrect ones.\n",
        "# Usage guidelines:\n",
        "# - Call this function only; do not rely on its internal implementation details.\n",
        "# - Gradients are not calculated within this function due to the `@torch.no_grad()` decorator.\n",
        "@torch.no_grad()\n",
        "def reward_oracle(x, pred):\n",
        "    return torch.where(true_y(x) == pred, 3, -1)"
      ],
      "metadata": {
        "id": "4Fxj50JRahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement Policy Gradient Algorithm:** Based on the formulas derived in part (b), complete the policy gradient implementation. For this task, we will use the Adam optimizer and process the full dataset in a single batch. The reward oracle has been invoked for you in the provided code. Remember, do not use the true labels directly in your implementation.\n",
        "\n",
        "*Hint: there are two approaches to get `logprob_on_predicted`: (a) use `torch.gather`. (b) use `nn.CrossEntropyLoss` (or its equivalent in `torch.nn.functional`), but with predicted labels instead of true labels.*"
      ],
      "metadata": {
        "id": "rCxct1tAahdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.002\n",
        "num_iters = 3000\n",
        "model = get_model(73)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "train_accs = []\n",
        "train_accs.append(accuracy(model, x, y))\n",
        "valid_accs = []\n",
        "valid_accs.append(accuracy(model, x_test, y_test))\n",
        "for epoch in range(num_iters):\n",
        "    logits = model(x)\n",
        "    logprob = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    predicted = logits.detach().argmax(dim=-1)\n",
        "    reward = reward_oracle(x, predicted)\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the loss function of policy gradient\n",
        "    ############################################################################\n",
        "    logprob_on_predicted = None\n",
        "\n",
        "    loss = None\n",
        "    ############################################################################\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_accs.append(accuracy(model, x, y))\n",
        "    valid_accs.append(accuracy(model, x_test, y_test))"
      ],
      "metadata": {
        "id": "Yp7o8V4cahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question\n",
        "\n",
        "**Include the screenshot of the accuracy plot** in your written assignment submission. With a correct implementation, you should observe a test accuracy of approximately 75% after the final iteration."
      ],
      "metadata": {
        "id": "dI6ah63CahdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i for i in range(len(train_accs))], train_accs, label=\"Train\")\n",
        "plt.plot([i for i in range(len(valid_accs))], valid_accs, label=\"Test\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(\"Final test accuracy:\", valid_accs[-1])"
      ],
      "metadata": {
        "id": "_r7vs-pLahdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question\n",
        "\n",
        "**Compare the policy gradient and supervised learning approaches for this classification task, focusing on their convergence speed, stability, and final performance. Explain any observed differences.** Include your response to this question in your written assignment submission."
      ],
      "metadata": {
        "id": "r5eGnPnuahdP"
      }
    }
  ]
}