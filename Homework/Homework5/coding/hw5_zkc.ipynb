{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSIC3UwxfT51"
      },
      "source": [
        "# Zachary's Karate Club"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hGTP-a3fT53"
      },
      "source": [
        "![zkc.png](https://i.imgur.com/CVHsMy6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDbx1uX9fT53"
      },
      "source": [
        "Zachary's karate club (ZKC) is a social network of a university karate club, described in the paper \"An Information Flow Model for Conflict and Fission in Small Groups\" by Wayne W. Zachary. \n",
        "\n",
        "The social network captures 34 members of a karate club, documenting links between pairs of members who interacted outside the club. \n",
        "\n",
        "During the study, a conflict arose between the officer/ administrator (\"John A\") and the instructor \"Mr. Hi\", which led to the split of the club into two. \n",
        "\n",
        "Part of the members formed a new club around Mr. Hi; and the remaining members went with the officer.\n",
        "\n",
        "Based on collected data Zachary correctly assigned all but one member of the club to the groups they actually joined after the split. You could read more about it here https://en.wikipedia.org/wiki/Zachary%27s_karate_club, here https://www.jstor.org/stable/3629752, and here https://commons.wikimedia.org/wiki/File:Social_Network_Model_of_Relationships_in_the_Karate_Club.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F3iYzIqfT54"
      },
      "source": [
        "#### Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71pPcLlpfT54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as ntwx\n",
        "from scipy.linalg import sqrtm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML as web\n",
        "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng3WqlwvfT54"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=100):\n",
        "    \"\"\"sets seed\"\"\"\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6wpWja5fT55"
      },
      "source": [
        "#### Processing Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR7P2voRfT55"
      },
      "outputs": [],
      "source": [
        "def get_graph_metadata(graph, key=None):\n",
        "    \"\"\"Return metadata obout a graph i.e. number_of_nodes, number_of_edges and node attributes\"\"\"\n",
        "    if key is None:\n",
        "        return graph.number_of_nodes(), graph.number_of_edges()\n",
        "    return graph.number_of_nodes(), graph.number_of_edges(), ntwx.get_node_attributes(graph, key)\n",
        "\n",
        "def get_xavier_init(input_dim, output_dim):\n",
        "    \"\"\"returns xaviers in initializer\"\"\"\n",
        "    std_dev = np.sqrt(5.0 / (input_dim + output_dim))\n",
        "    return np.random.uniform(-std_dev, std_dev, size=(input_dim, output_dim))\n",
        "\n",
        "def get_cross_entropy(pred, labels):\n",
        "    \"\"\"computes crossentropy between the predictions and the labels\"\"\"\n",
        "    return -np.log(pred)[np.arange(pred.shape[0]), np.argmax(labels, axis=1)]\n",
        "\n",
        "\n",
        "def normalized_difference_norm(dW, dW_approx):\n",
        "    \"\"\"compares the deirivarive of the weight with its apprimation\"\"\"\n",
        "    return np.linalg.norm(dW - dW_approx) / (np.linalg.norm(dW) + np.linalg.norm(dW_approx))\n",
        "\n",
        "def get_colors_labels_and_classes(graph, num_nodes):\n",
        "    \"\"\"We applied greedy modurality maximization from the original\n",
        "    paper https://arxiv.org/pdf/1609.02907.pdf to come up with cluster\n",
        "    labels for each of the members of the club. We will train our GNN\n",
        "    to predict these cluster labels.\"\"\"\n",
        "    clusters = greedy_modularity_communities(graph)\n",
        "    unsure_cluster = clusters[-1]\n",
        "    clusters = clusters[:2]\n",
        "    clusters[1] = clusters[1].union(unsure_cluster)\n",
        "    color_lists = np.zeros(num_nodes)\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        color_lists[list(cluster)] = i\n",
        "    classes = np.unique(color_lists).shape[0]\n",
        "    return color_lists, np.eye(classes)[color_lists.astype(int)], classes\n",
        "\n",
        "def get_affiliation(club_labels):\n",
        "    \"\"\"return the affiation of the karate club members\"\"\"\n",
        "    Mr_Hi, Officer = [], []\n",
        "    for key, value in club_labels.items():\n",
        "        if value == 'Mr. Hi':\n",
        "            Mr_Hi.append(key)\n",
        "        else:\n",
        "            Officer.append(key)\n",
        "    return Mr_Hi, Officer\n",
        "\n",
        "\n",
        "def fill_diagonal(source_array, diagonal):\n",
        "    \"\"\"helps fill element of the source array into a diagonal matrix\"\"\"\n",
        "    copy = source_array.copy()\n",
        "    np.fill_diagonal(copy, diagonal)\n",
        "    return copy\n",
        "                                          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfp-6iPsfT55"
      },
      "source": [
        "The original paper on greedy modularity communities maximization could be found here https://journals.aps.org/pre/pdf/10.1103/PhysRevE.70.066111?casa_token=Fqnjw_t-J64AAAAA%3ADmyzj146CDE-UeW_1I6Ifvu40GmCC_goDC4i6lvkYIa9GENKcktHxOgHO5et7Z7xJ3NU1q2Ngt2J6Zs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEk6IxLjfT56"
      },
      "source": [
        "#### Visualization Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxBpkVlJfT56"
      },
      "outputs": [],
      "source": [
        "def show_graph(graph,\n",
        "               label_values_of_nodes,\n",
        "               label_colors_of_nodes,\n",
        "               colors_of_edges='black',\n",
        "               display_window_size=15,\n",
        "               positions_of_nodes=None,\n",
        "               cmap='jet'):\n",
        "    \"\"\"helps visualize the graph\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(display_window_size, display_window_size))\n",
        "    if positions_of_nodes is None:\n",
        "        # https://networkx.org/documentation/stable/reference/generated/networkx.drawing.layout.spring_layout.html\n",
        "        positions_of_nodes = ntwx.spring_layout(graph,\n",
        "                                                k=5/np.sqrt(graph.number_of_nodes()))\n",
        "        # https://networkx.org/documentation/stable/reference/drawing.html\n",
        "    ntwx.draw(\n",
        "        graph,\n",
        "        positions_of_nodes,\n",
        "        with_labels=label_values_of_nodes, \n",
        "        labels=label_values_of_nodes, \n",
        "        node_color=label_colors_of_nodes, \n",
        "        ax=ax,\n",
        "    cmap=cmap,\n",
        "    edge_color=colors_of_edges)\n",
        "    \n",
        "    \n",
        "def plot_training_curves(train_losses, test_losses, accs, grid=False):\n",
        "    \"\"\"shows training curves\"\"\"\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.plot(np.log10(train_losses), label='train')\n",
        "    ax1.plot(np.log10(test_losses), label='test')\n",
        "    ax1.legend()\n",
        "    if grid:\n",
        "        ax1.grid()\n",
        "    \n",
        "    ax2 = fig.add_subplot(122)\n",
        "    ax2.plot(accs, label='acc')\n",
        "    ax2.set(ylim=[0,1])\n",
        "    ax2.legend()\n",
        "    \n",
        "    if grid:\n",
        "        ax2.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgQjYGVGfT56"
      },
      "source": [
        "#### Gradient Update Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW6EJthBfT56"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_gradients(param_name, layer, inputs_data, gt_labels, eps=1e-4, weight_decay=0):\n",
        "    \"\"\"Compute the gradient with respect to a a given parameter in a given layer\"\"\"\n",
        "    gradients_utils = {}\n",
        "    batch_size = gt_labels.shape[0]\n",
        "    replica = getattr(layer, param_name).copy()\n",
        "    replica_flattened = np.asarray(replica).flatten()\n",
        "    gradients_utils['gradient_values'] = np.zeros(replica_flattened.shape)\n",
        "    n_parms = replica_flattened.shape[0]\n",
        "    for ind, param in enumerate(replica_flattened):\n",
        "        # lower bound cost\n",
        "        replica_flattened[ind] = param - eps\n",
        "        temp = replica_flattened.reshape(replica.shape)\n",
        "        gradients_utils['lower_bound_pred'] = layer.forward_pass(*inputs_data, **{param_name: temp})\n",
        "        decay = weight_decay/ 2 * np.sum(replica_flattened ** 2) / batch_size\n",
        "        lower_cost = np.mean(get_cross_entropy(gradients_utils['lower_bound_pred'],\n",
        "                                               gt_labels)) + decay\n",
        "        # upper bound cost\n",
        "        replica_flattened[ind] = param + eps\n",
        "        temp = replica_flattened.reshape(replica.shape)\n",
        "        gradients_utils['upper_bound_pred'] = layer.forward_pass(*inputs_data, **{param_name: temp})\n",
        "        decay = weight_decay/ 2 * np.sum(replica_flattened**2) / batch_size\n",
        "        \n",
        "        upper_cost = np.mean(get_cross_entropy(gradients_utils['upper_bound_pred'],\n",
        "                                               gt_labels)) + decay\n",
        "        gradients_utils['gradient_values'][ind] = ((upper_cost - lower_cost) / (2 * eps))\n",
        "        replica_flattened[ind] = param\n",
        "    return gradients_utils['gradient_values'].reshape(replica.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ0fD-g8fT57"
      },
      "source": [
        "#### Grad Descent Optimizer Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhLQixVlfT57"
      },
      "outputs": [],
      "source": [
        "class Grad_Descent_Optimizer():\n",
        "    \"\"\"Performs Gradient Descent\"\"\"\n",
        "    def __init__(self, learning_rate, weight_decay):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self._y_pred = None\n",
        "        self._y_true = None\n",
        "        self._output = None\n",
        "        self.batch_size = None\n",
        "        self.nodes_to_be_trained = None\n",
        "        \n",
        "    def __call__(self, y_pred, y_true, nodes_to_be_used_for_training=None):\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        self.batch_size = y_pred.shape[0]\n",
        "        \n",
        "        if nodes_to_be_used_for_training is None:\n",
        "            self.nodes_to_be_used_for_training = np.arange(self.batch_size)\n",
        "        else:\n",
        "            self.nodes_to_be_used_for_training = nodes_to_be_used_for_training\n",
        "        \n",
        "    @property\n",
        "    def out(self, ):\n",
        "        return self._output\n",
        "    \n",
        "    @out.setter\n",
        "    def out(self, y):\n",
        "        self._output = y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5zZKJlSfT57"
      },
      "source": [
        "#### Set seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya7z7m3PfT57"
      },
      "outputs": [],
      "source": [
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQO87b3fT57"
      },
      "source": [
        "#### Implementation Check Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE3Xp7JlfT57"
      },
      "outputs": [],
      "source": [
        "def implementation_check(dW, dW_approx, db, db_approx):\n",
        "    \"\"\"This function helps check how correct in your impplementation a layer\"\"\"\n",
        "    try:\n",
        "        assert normalized_difference_norm(dW, dW_approx) < 1e-7\n",
        "        assert normalized_difference_norm(db, db_approx) < 1e-7\n",
        "        print('congrats, your implementation passes the test !!!')\n",
        "    except:\n",
        "        print('Not quite there :( yet; your implementation did not pass the test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0juNZhZfT57"
      },
      "source": [
        "#### Training Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLT0APXcfT57"
      },
      "outputs": [],
      "source": [
        "def train_test_split(test_nodes, labels):\n",
        "    return np.array([i for i in range(labels.shape[0]) if i not in test_nodes])\n",
        "\n",
        "def threshold(arr, threshold_value=0.5):\n",
        "    \"\"\"This function treshold the output of a softmax to either be 0 or 1\"\"\"\n",
        "    arr[arr>=threshold_value]=1\n",
        "    arr[arr<threshold_value]=0\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujEcL3JvfT57"
      },
      "source": [
        "#### Node Classification Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woNVPpCkfT57"
      },
      "outputs": [],
      "source": [
        "class Softmax_Layer():\n",
        "    \"\"\"applies a weight multiplication and returns the forward and backward passes softmax. Some values are cached \n",
        "    so we could use them to compute the gredients.\n",
        "    Returns: (batch_size, output_dim)\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, name=''):\n",
        "        self.name = name\n",
        "        self.cache = {}\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.bias = np.zeros((self.output_dim, 1))\n",
        "        self.W = get_xavier_init(self.output_dim, self.input_dim)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        dims = (self.input_dim, self.output_dim)\n",
        "        if self.name:\n",
        "            return f\"Softmax_Layer: W{'_' + self.name}{dims}\"\n",
        "        else:\n",
        "            return f\"Softmax_Layer: W{'_'+ ''}{dims}\"\n",
        "        \n",
        "    def get_softmax(self, x):\n",
        "        x = x - np.max(x, axis=0, keepdims=True)\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
        "        \n",
        "    def forward_pass(self, X, W=None, bias=None):\n",
        "        \"\"\"Returns the softmax of the input X after appliying the weight and biases.\"\"\"\n",
        "        self.cache['X'] = X.T\n",
        "        if W is None:\n",
        "            W = self.W\n",
        "        if bias is None:\n",
        "            bias = self.bias\n",
        "\n",
        "        return self.get_softmax(np.asarray(W @ self.cache['X']) + bias).T # (batch_size, output_dim)\n",
        "    \n",
        "    def backward_pass(self, optimizer, need_update=True):\n",
        "        \"\"\"mask nodes nodes not revelant for training, and updates optimizer parameters\"\"\"\n",
        "        training_mask = np.zeros(optimizer.y_pred.shape[0])\n",
        "        training_mask[optimizer.nodes_to_be_trained] = 1\n",
        "        training_mask = training_mask.reshape((-1, 1))\n",
        "        \n",
        "        dLoss = np.asarray((optimizer.y_pred - optimizer.y_true)) \n",
        "        dLoss = np.multiply(dLoss, training_mask)\n",
        "        \n",
        "        self.grad = dLoss @ self.W # (batch_size, input_dim)\n",
        "        optimizer.output = self.grad\n",
        "        \n",
        "        dW = (dLoss.T @ self.cache['X'].T) / optimizer.batch_size # (output_dim, input_dim)\n",
        "        dbias = np.sum(dLoss.T, axis=1, keepdims=True) / optimizer.batch_size # (output_dim, 1)\n",
        "                \n",
        "        dW_weight_decay = self.W * optimizer.weight_decay / optimizer.batch_size\n",
        "        \n",
        "        if need_update:   \n",
        "            self.W = self.W - (dW + dW_weight_decay) * optimizer.learning_rate\n",
        "            self.bias = self.bias - dbias.reshape(self.bias.shape) * optimizer.learning_rate\n",
        "        \n",
        "        return dW + dW_weight_decay, dbias.reshape(self.bias.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gUoVbK_fT58"
      },
      "source": [
        "### ZKC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Z6W9DwfT58"
      },
      "source": [
        "We will train a GNN to cluster people in the karate club in such that people who are more likely to associate with either the officer or Mr. Hi will be close together, while the distance beween the 2 classes will be far.\n",
        "\n",
        "In the original paper titled \"Semi-Supervised Classification with\n",
        "Graph Convolutional Networks\" that can be found here https://arxiv.org/pdf/1609.02907.pdf, the authors framed this as a node-level classification problem on a graph. We will pretend that we only know the affiliation labels for some of the nodes (which we'll call our training set) and we'll predict the affiliation labels for the rest of the nodes (our test set).\n",
        "\n",
        "We will build a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:\n",
        "\n",
        "$$H^{(l+1)} = σ(D^{-1/2}\\tilde{A} D^{-1/2}H^{(l)}W^{(l)})\n",
        "            = σ(\\tilde{A}^{SymNorm}H^{(l)}W^{(l)})$$\n",
        "\n",
        "where $\\tilde{A}$ is the adjacency matrix that we discussed in Homework 4, except that now we add the identity to include self loops at every node for numerical stability. $D$ is the degree matrix as defined in the past, $H^{(l)}$ is the activation matrix of the $l$-th layer, and $W$ is the weight matrix to be learned. $\\sigma$ is an activation function, in this case tanh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmMNOTQhfT58"
      },
      "source": [
        "We used the python module networkx to import the dataset and provided some helper functions to help understand the data as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ_QNowZfT58"
      },
      "outputs": [],
      "source": [
        "graph = ntwx.karate_club_graph()\n",
        "num_nodes, num_edges, club_labels = get_graph_metadata(graph, 'club')\n",
        "colors, labels, num_classes = get_colors_labels_and_classes(graph, num_nodes)\n",
        "Mr_Hi_people, Officer_people = get_affiliation(club_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgwmM2NfT58"
      },
      "source": [
        "### Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YP9PHikfT58"
      },
      "outputs": [],
      "source": [
        "print(f'ZKC dataset graph has {num_nodes} nodes and {num_edges} edges')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe1C2CCofT58"
      },
      "outputs": [],
      "source": [
        "print(f'The affiation of each of the 34 members between the officer and Mr. Hi is given below:\\n {club_labels}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27K_KJzDfT58"
      },
      "outputs": [],
      "source": [
        "print(f'nodes/people loyal to Mr. Hi are:\\n {Mr_Hi_people}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wmAem3EfT58"
      },
      "outputs": [],
      "source": [
        "print(f'nodes/people loyal to the officer are:\\n {Officer_people}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6CJyCE4fT58"
      },
      "outputs": [],
      "source": [
        "assert len(Officer_people) + len(Mr_Hi_people) == num_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4YfCa8fT58"
      },
      "source": [
        "Note that the nodes represent the people and the edges represent the social interaction between the people outside on the club. Also the colors assigned to the nodes as shown below are indepedent of the class since no model has been trained to properly do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htnq-cU0fT59"
      },
      "source": [
        "##### Show Graph with Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngrXcl0YfT59"
      },
      "outputs": [],
      "source": [
        "show_graph(graph=graph,\n",
        "           label_values_of_nodes=club_labels,\n",
        "           label_colors_of_nodes=colors,\n",
        "           colors_of_edges='black',\n",
        "           positions_of_nodes=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV_sWXNmfT59"
      },
      "source": [
        "##### Show Graph without Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-C18OIefT59"
      },
      "outputs": [],
      "source": [
        "show_graph(graph=graph,\n",
        "           label_values_of_nodes=None,\n",
        "           label_colors_of_nodes=colors,\n",
        "           colors_of_edges='black',\n",
        "           positions_of_nodes=None,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xHbQa0lfT59"
      },
      "source": [
        "Note that node labels are obtained using the greedy modurality maximization algorithm descibted in the paper linked above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEPnsGjJfT59"
      },
      "source": [
        "Our goal in this problem is therefore to write a simple Graph Neural Network using python to perform node classification. We will also use the node embedding to move nodes with similar classes close to each other. We have provided here the adjacency matrix of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uidJT6S9fT59"
      },
      "outputs": [],
      "source": [
        "A = ntwx.to_numpy_array(graph)\n",
        "A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pXC7gmMfT59"
      },
      "source": [
        "Note that we adjacency matrix does not include the node itself. We want our network to be aware of information about the nodes themselves instead of only the neighborhood, so we add self loops our adjacency matrix. The paper called this $\\tilde A$.\n",
        "\n",
        "#### Q. 1. Compute: $$\\tilde{A}=A_{self-loop}= A + I$$\n",
        "Where I is the identity matrix which allows us to include self loops of each nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNYlFfEwfT59"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# A_tild = ????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xygBnJwufT59"
      },
      "source": [
        "#### Q.2. Write a function that takes in $\\tilde A$ as argument and returns the $\\tilde A^{SymNorm}$ adjacency matrix. You may find the provided function `fill_diagonal` useful, as well as the inverse function `np.linalg.inv` and the matrix square root function `scipy.linalg.sqrtm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1krH_MS-fT59"
      },
      "outputs": [],
      "source": [
        "def get_adjacency_matrix(A_tild):\n",
        "    # TODO\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKVRFpmxfT59"
      },
      "outputs": [],
      "source": [
        "A_hat = get_adjacency_matrix(A_tild)\n",
        "A_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN7P_iZWfT59"
      },
      "source": [
        "The other input to our GNN is the graph node matrix $X$ which contains node features. For simplicity, we set $X$ to be the identity matrix because we don't have any node features in this example. In a sense, this will map each node in the graph to a column of learnable parameters in the first layer, resulting in a fully learnable node embedings. In the question below, set the matrix $X$ to be the identity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGniE2gBfT59"
      },
      "source": [
        "##### Q.3. Generate the feature input matrix $X$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy4eyPLdfT59"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# X = ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w27d7hH6fT59"
      },
      "source": [
        "#### Single GNN Layer Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mobFx2X_fT59"
      },
      "source": [
        "We will first implement a single layer GNN. Using the equation provided in the paper that is mentioned above, implement a forward and backward pass for a simple GNN layer. \n",
        "\n",
        "Note that for $l$=0, $H$ is the input $X$ and $\\tilde A H$ does the message passing as we have seen in the previous homework and discussion, which is in turn multiplied by a weight matrix $W$. A non linearity is therefore applied afterward.\n",
        "\n",
        "\n",
        "In the backward pass, we will apply L2 regularization to the weight matrix $W$. The regularization term is defined as: $\\lambda \\sum_{i,j} W_{i,j}^2$ which has gradient: $\\lambda 2W$. We will combine $\\lambda 2$ in the weight decay parameter optimizer.weight_decay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch46MzopfT5-"
      },
      "source": [
        "#### Q.4. Complete the #TODO to implement a forward pass that does just that in the class below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUzghotBfT5-"
      },
      "outputs": [],
      "source": [
        "class GNN_Layer():\n",
        "    \"\"\"process a single GNN layer\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, name=''):\n",
        "        self.name = name\n",
        "        self.cache = {}\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.W = get_xavier_init(self.output_dim, self.input_dim)\n",
        "        self.activation = np.tanh\n",
        "        \n",
        "    def __repr__(self):\n",
        "        dims = (self.input_dim, self.output_dim)\n",
        "        if self.name:\n",
        "            return f\"GNN_Layer: W{'_' + self.name}{dims}\"\n",
        "        else:\n",
        "            return f\"GNN_Layer: W{'_'+ ''}{dims}\"\n",
        "        \n",
        "    def forward_pass(self, A, X, W=None):\n",
        "        \"\"\"A here is the symmetricaly normalized adjacency matrix\n",
        "        and X is the input to the layer. We cached some values \n",
        "        to use in the backward pass.\"\"\"\n",
        "        \n",
        "        self.cache['A'] = A # (batch_size, batch_size)\n",
        "        # TODO \n",
        "        # self.cache['X'] = ?? # (batch_size, input_node_feature_dim)\n",
        "        \n",
        "        # if W is None:\n",
        "            # W = ??\n",
        "        \n",
        "        # H = ?? (batch_size, hidden_dim)\n",
        "        # H = ?? [apply activation] (batch_size, hidden_dim)\n",
        "        # self.cache['H'] = ?? # (batch_size, hidden_dim)\n",
        "        return # (batch_size, hidden_dim)\n",
        "    \n",
        "    def backward_pass(self, optimizer, need_update=True):\n",
        "        # dtanh = ?? # (batch_size, output_dim)\n",
        "        # optimizer.output contains the gradient from the next layer. It is multiplied by the batch size,\n",
        "        # so you'll need to divide by optimizer.batch_size to get the average gradient.\n",
        "        d = np.multiply(optimizer.output, dtanh) # (batch_size, output_dim)\n",
        "        \n",
        "        # self.grad = ?? (batch_size, input_dim)\n",
        "        \n",
        "        optimizer.output = self.grad\n",
        "        \n",
        "        # dW = ?? # (output_dim, input_node_feature_dim)\n",
        "        # dW_weight_decay = ??\n",
        "        \n",
        "        # if need_update: # Use the gradient descent update rule on W. Remember to include weight decay.\n",
        "            # self.W = ??\n",
        "        \n",
        "        return # (output_dim, input_node_feature_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7kvgdnJfT5-"
      },
      "source": [
        "#### We now test the your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK7gktJlfT5-"
      },
      "source": [
        "lets's instantiate the GNN_Layer Class and the Softmax_Layer provided in the helper functions to test the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiXorvuRfT5-"
      },
      "outputs": [],
      "source": [
        "gnn_layer = GNN_Layer(input_dim=num_nodes,\n",
        "                output_dim=2,\n",
        "                name='gnn_layer_1')\n",
        "\n",
        "sm_layer = Softmax_Layer(input_dim=2,\n",
        "                   output_dim=num_classes,\n",
        "                   name='softmax_layer')\n",
        "\n",
        "optim = Grad_Descent_Optimizer(learning_rate=0,\n",
        "                               weight_decay=1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xMPF1CHfT5-"
      },
      "source": [
        "####  Q. 5. lets compute the forward passes, uncomment and complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQtMchu1fT5-"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# gnn_layer_output = gnn_layer.forward_pass(A=??,\n",
        "#                                           X=??)\n",
        "\n",
        "# optim(y_pred=sm_layer.forward_pass(X=gnn_layer_output), y_true=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT4tJhmdfT5-"
      },
      "source": [
        "lets verify that the layers are properly implemented by looking at the gradients. Note that need update is used only when we are updating the parameters during training. We do not need to do so here since we are just testing our layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCeLG4dsfT5-"
      },
      "outputs": [],
      "source": [
        "dW_approx = compute_gradients(param_name=\"W\",\n",
        "                              layer=sm_layer,\n",
        "                              inputs_data=(gnn_layer_output,),          \n",
        "                              gt_labels=labels,\n",
        "                              eps=1e-4,\n",
        "                              weight_decay=optim.weight_decay)\n",
        "\n",
        "db_approx = compute_gradients(param_name=\"bias\",\n",
        "                              layer=sm_layer,\n",
        "                              inputs_data=(gnn_layer_output,),\n",
        "                              gt_labels=labels,\n",
        "                              eps=1e-4,\n",
        "                              weight_decay=optim.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huY3jXjzfT5-"
      },
      "outputs": [],
      "source": [
        "dW, db = sm_layer.backward_pass(optim, need_update=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y2b_jA9fT5-"
      },
      "source": [
        "We then get the gradients of Softmax layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGPoMazPfT5-"
      },
      "outputs": [],
      "source": [
        "dW, db = sm_layer.backward_pass(optim, need_update=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcloB40LfT5-"
      },
      "source": [
        "#### We now assert that the true and approximate gradients are very close to each other. If not, something went wrong in our implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipIK1nQ3fT5-"
      },
      "outputs": [],
      "source": [
        "implementation_check(dW, dW_approx, db, db_approx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID1QXwbxfT5-"
      },
      "source": [
        "#### Q. 6. Now,  use the GNN and Softmax layers implemented above to set up our GNN Network. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBKy7XD3fT5-"
      },
      "outputs": [],
      "source": [
        "class GNN():\n",
        "    \"\"\"This class leverages the GNN layer implemanted above by cascading them into a layer.\"\"\"\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 hidden_dim,\n",
        "                 num_layers,):\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = []\n",
        "#         first_gnn_layer = GNN_Layer(input_dim=??,\n",
        "#                                   output_dim=??,\n",
        "#                                   name='layer_0')\n",
        "        \n",
        "        self.layers.append(first_gnn_layer)\n",
        "        \n",
        "#         for layer in range(num_layers):\n",
        "#             gnn_temp = GNN_Layer(input_dim=??,\n",
        "#                                  output_dim=??,\n",
        "#                                  name=f'layer_{layer}')\n",
        "#             self.layers.append(gnn_temp)\n",
        "            \n",
        " \n",
        "#         last_gnn_layer = Softmax_Layer(input_dim=??,\n",
        "#                                output_dim=??,\n",
        "#                                name='sm_layer')\n",
        "#         self.layers.append(last_gnn_layer)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([str(layer) for layer in self.layers])\n",
        "    \n",
        "    def embedding(self, A, X):\n",
        "        H = X\n",
        "        for layer in self.layers[:-1]:\n",
        "            # H = ??\n",
        "            pass\n",
        "        # return \n",
        "    \n",
        "    def forward_pass(self, A, X):\n",
        "        # H = ??\n",
        "        out = self.layers[-1].forward_pass(H)\n",
        "        pass\n",
        "        # return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1smpQc3fT5_"
      },
      "source": [
        "#### Q.7. Let's initialize our model! Uncomment the code bleow and the correct input and output dimensions to initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPMokiThfT5_"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# gnn_model = GNN(\n",
        "#     input_dim=??, \n",
        "#     output_dim=??, \n",
        "#     num_layers=2,\n",
        "#     hidden_dim=[16, 2], \n",
        "# )\n",
        "# gnn_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9DIaycfT5_"
      },
      "source": [
        "#### Train/ Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWcBpDHLfT5_"
      },
      "source": [
        "We chose nodes 0, 1, 8, 3, 8, 15, 16, 20, 25, 28, and 30 t to be our test nodes and used all the remaining for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDfA1JHafT5_"
      },
      "outputs": [],
      "source": [
        "test_nodes = np.array([0, 1, 8, 3, 8, 15, 16, 20, 25, 28, 30])\n",
        "train_nodes = train_test_split(test_nodes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lynqScU_fT5_"
      },
      "outputs": [],
      "source": [
        "print(f'The nodes that will be used for training are:\\n {train_nodes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qt1vHUFfT5_"
      },
      "outputs": [],
      "source": [
        "print(f'The nodes that will be used for test/val are:\\n {test_nodes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vuOz1agfT5_"
      },
      "source": [
        "We now instantiate our optimizer with a learning rate and weight decay for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35-gsVuNfT5_"
      },
      "outputs": [],
      "source": [
        "training_optim = Grad_Descent_Optimizer(learning_rate=2e-2, weight_decay=2.5e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm9RtqJ8fT5_"
      },
      "source": [
        "#### Q.8 Complete the training loop function below.  Note that the train loss and test loss are computed over a given set of nodes that is defined by our training and testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwFykFJGfT5_"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          A_hat,\n",
        "          X,\n",
        "          y_true,\n",
        "          nodes_to_be_used_for_training,\n",
        "          nodes_to_be_used_for_testing,\n",
        "          threshold_value=.5,\n",
        "          ealy_stopping_steps=60,\n",
        "          num_epochs=20000):\n",
        "    \"\"\"trains our gnn model\"\"\"\n",
        "    \n",
        "    accs = []\n",
        "    training_losses = []\n",
        "    testing_losses = []\n",
        "    embeddings = []\n",
        "    y_preds = []\n",
        "\n",
        "    minimum_loss = 1e7\n",
        "    ealy_stopping_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # TODO\n",
        "        # y_pred = ??\n",
        "        training_optim(y_pred, y_true, nodes_to_be_used_for_training)\n",
        "\n",
        "        for layer in reversed(model.layers):\n",
        "            layer.backward_pass(training_optim, need_update=True)\n",
        "\n",
        "        embeddings.append(model.embedding(A_hat, X))\n",
        "        y_preds.append(y_pred)\n",
        "        acc_temp = (np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))[\n",
        "            [i for i in range(y_true.shape[0]) if i not in nodes_to_be_used_for_training]\n",
        "        ]\n",
        "        accs.append(np.mean(acc_temp))\n",
        "\n",
        "        # loss_temp = ??\n",
        "        # train_loss_temp = ??\n",
        "        # test_loss_temp =??\n",
        "\n",
        "        training_losses.append(train_loss_temp)\n",
        "        testing_losses.append(test_loss_temp)\n",
        "\n",
        "        if test_loss_temp < minimum_loss:\n",
        "            minimum_loss = test_loss_temp\n",
        "            ealy_stopping_counter = 0\n",
        "        else:\n",
        "            ealy_stopping_counter += 1\n",
        "\n",
        "        if ealy_stopping_counter > ealy_stopping_steps:\n",
        "            print(\"Training Stopped due to Early stopping!\")\n",
        "            break\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"epoch #: {epoch+1} \\t| train Loss: {train_loss_temp:.3f} \\t| test Loss: {test_loss_temp:.3f}\")\n",
        "\n",
        "    training_losses = np.array(training_losses)\n",
        "    testing_losses = np.array(testing_losses)\n",
        "    y_preds = threshold(np.array(y_preds), threshold_value)\n",
        "    return training_losses, testing_losses, accs, embeddings, y_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls2Mfd2YfT5_"
      },
      "outputs": [],
      "source": [
        "training_losses, testing_losses, accs, embeddings, preds = train(model=gnn_model,\n",
        "                                  A_hat=A_hat,\n",
        "                                  X=X,\n",
        "                                  y_true=labels,\n",
        "                                  nodes_to_be_used_for_training=train_nodes,\n",
        "                                  nodes_to_be_used_for_testing=test_nodes,\n",
        "                                  ealy_stopping_steps=50,\n",
        "                                  num_epochs=20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eej-JdMTfT5_"
      },
      "outputs": [],
      "source": [
        "print(f'For this toy example, the classification accuracy on the test set is {accs[-1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2nr8kL0fT5_"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_losses, testing_losses, accs, grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JyaeI21fT5_"
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfft5O6TfT5_"
      },
      "source": [
        "Let's observe the affiliation of people in our test nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d1u5ae-fT5_"
      },
      "outputs": [],
      "source": [
        "[club_labels[i] for i in test_nodes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH6vKazKfT5_"
      },
      "source": [
        "Let's observe the position where our trainined model predict them to be at."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogz-r14RfT5_"
      },
      "outputs": [],
      "source": [
        "embeddings[-1][test_nodes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mIj3sRcfT6A"
      },
      "source": [
        "Let's observe the GT for values for the test nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcmHq0ZTfT6A"
      },
      "outputs": [],
      "source": [
        "labels[test_nodes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAiT8Ol0fT6A"
      },
      "source": [
        "Let's observe the predicted values for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjBpxTpRfT6A"
      },
      "outputs": [],
      "source": [
        "preds[-1][test_nodes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc8xdvg7fT6A"
      },
      "source": [
        "Finally, lets observe all the data clustered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TEdNXiZfT6A"
      },
      "outputs": [],
      "source": [
        "show_graph(graph=graph,\n",
        "           label_values_of_nodes=club_labels,\n",
        "           label_colors_of_nodes=colors,\n",
        "           colors_of_edges='black',\n",
        "           positions_of_nodes={i: embeddings[-1][i,:] \\\n",
        "                               for i in range(embeddings[-1].shape[0])},\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfq53EUPfT6A"
      },
      "source": [
        "#### Q. 9. Explain why we obtain a 100% on accuracy on our test set, yet we see in the plot above that 2 samples seem to be misclassified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_DdrZsGfT6A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}