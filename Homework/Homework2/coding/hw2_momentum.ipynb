{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "600af611-161a-41fb-87fa-1a3188c3b35d",
      "metadata": {
        "id": "600af611-161a-41fb-87fa-1a3188c3b35d"
      },
      "source": [
        "# CS182 HW2 - Accelerating Gradient Descent with Momentum (coding part)\n",
        "\n",
        "In this notebook, we will understand gradient descent and  gradient descent with momentum in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "180f3378-b2cf-44c1-91c6-d4961d282cdb",
      "metadata": {
        "id": "180f3378-b2cf-44c1-91c6-d4961d282cdb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3c2d38-1039-408c-a3a5-7efb371fec28",
      "metadata": {
        "id": "ec3c2d38-1039-408c-a3a5-7efb371fec28"
      },
      "source": [
        "## Generate and Visualize Data\n",
        "\n",
        "We generate a dataset of 2D datapoints from the gaussian distribution with a mean of $(-3, 0)$ and covariance matrix of $\\begin{pmatrix}3 & 0 \\\\ 0 & 1\\end{pmatrix}$. The binary labels $y$ indicate whether the second dimension is greater than 0 (positive) or not (negative). The data is visualized using a scatter plot with different colors representing the different labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743c76f1-6f99-4ad7-b7b2-f32a17d5229e",
      "metadata": {
        "id": "743c76f1-6f99-4ad7-b7b2-f32a17d5229e"
      },
      "outputs": [],
      "source": [
        "def gen_gaussian_points(n, mean, sigma):\n",
        "    return np.random.normal(mean, sigma, [n, 2])\n",
        "\n",
        "N = 500\n",
        "\n",
        "X = gen_gaussian_points(N, [-3, 0], [3, 1])\n",
        "y = (X[:,1]>0).astype(float)\n",
        "y = np.expand_dims(y, axis=-1)\n",
        "\n",
        "\n",
        "plt.scatter(*X[y.squeeze()==0].T)\n",
        "plt.scatter(*X[y.squeeze()==1].T)\n",
        "plt.title(\"Visualization of Data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oDuTuhNajI_z",
      "metadata": {
        "id": "oDuTuhNajI_z"
      },
      "source": [
        "## Visualizing the Loss Landscape\n",
        "\n",
        "The following contour plot visualizes the loss landscape of this optimization task. It's important to note that the data has been generated such that the correlation coefficient between dimension 0 and dimension 1 is zero, aligning dimension 0 and dimension 1 with the two singular vectors in the Singular Value Decomposition (SVD) of the data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MVjQIX8Mjt2t",
      "metadata": {
        "id": "MVjQIX8Mjt2t"
      },
      "outputs": [],
      "source": [
        "w0_s, w1_s = np.meshgrid(np.linspace(-0.5, 0.5, 100), np.linspace(-0.5, 0.5, 100))\n",
        "w_s = np.stack([w0_s.reshape(-1), w1_s.reshape(-1)], axis=1)\n",
        "loss_s = ((X @ w_s.T - y) ** 2).sum(axis=0).reshape(100, 100)\n",
        "from matplotlib import ticker, cm\n",
        "plt.contourf(w0_s, w1_s, loss_s, cmap=cm.PuBu_r, levels=40)\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"w0\")\n",
        "plt.ylabel(\"w1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3e8be6-c6d6-4fb2-946a-b4d6530fe5a6",
      "metadata": {
        "id": "ce3e8be6-c6d6-4fb2-946a-b4d6530fe5a6"
      },
      "source": [
        "## (Plain) Gradient Descent\n",
        "We will implement gradient descent *without* momentum below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ecbc62-3958-46e3-aeea-167e4fefcd2a",
      "metadata": {
        "id": "08ecbc62-3958-46e3-aeea-167e4fefcd2a"
      },
      "outputs": [],
      "source": [
        "def runGD(maxiter,stepsize):\n",
        "    w = np.zeros((2, 1))\n",
        "    grads = []\n",
        "    ws = []\n",
        "    losses = []\n",
        "    for i in range(maxiter):\n",
        "        grad = 2 * (X.T @ X @ w) - 2 * X.T @ y\n",
        "        w = w - stepsize * grad\n",
        "        grads.append(grad)\n",
        "        ws.append(w)\n",
        "        loss = np.linalg.norm(y - X @ w) ** 2\n",
        "        losses.append(loss)\n",
        "    print(\"Final loss =\", loss)\n",
        "    grads = np.array(grads).squeeze()\n",
        "    ws = np.array(ws).squeeze()\n",
        "    return grads, ws, losses \n",
        "\n",
        "maxiter = 100\n",
        "stepsize = 1e-4\n",
        "grads, ws, losses = runGD(maxiter,stepsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f85f532-3ac4-402e-9c96-2f0c76acd9c0",
      "metadata": {
        "id": "6f85f532-3ac4-402e-9c96-2f0c76acd9c0"
      },
      "source": [
        "## Gradient Descent with Momentum\n",
        "Implement the gradient descent with momentum algorithm. **Fill in the missing code** for updating the parameters. As a verification step, compare the final loss with the previous part to ensure it is reasonable and not significantly different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fded0aa-b390-4f67-8522-fe924ccccf02",
      "metadata": {
        "id": "9fded0aa-b390-4f67-8522-fe924ccccf02"
      },
      "outputs": [],
      "source": [
        "def runGDM(maxiter, stepsize, beta):\n",
        "    w = np.zeros((2, 1))\n",
        "    grads_m = []\n",
        "    ws_m = []\n",
        "    losses_m = []\n",
        "    for i in range(maxiter):\n",
        "        grad = 2 * (X.T @ X @ w) - 2 * X.T @ y\n",
        "        if i == 0:\n",
        "            smoothed_grad = grad\n",
        "        ###############################################\n",
        "        ###       TODO: YOUR CODE HERE              ###\n",
        "        ###############################################\n",
        "        smoothed_grad = ?\n",
        "        ###############################################\n",
        "        ###       END OF YOUR CODE                  ###\n",
        "        ###############################################\n",
        "        w = w - stepsize * smoothed_grad\n",
        "        grads_m.append(grad)\n",
        "        ws_m.append(w)\n",
        "        loss = np.linalg.norm(y - X @ w) ** 2\n",
        "        losses_m.append(loss)\n",
        "    print(\"Final loss =\", loss)\n",
        "    grads_m = np.array(grads_m).squeeze()\n",
        "    ws_m = np.array(ws_m).squeeze()\n",
        "    return grads_m, ws_m, losses_m\n",
        "\n",
        "maxiter = 100\n",
        "stepsize = 1e-4\n",
        "beta = 0.6\n",
        "grads_m, ws_m, losses_m = runGDM(maxiter, stepsize, beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d525ecbf-7348-477f-a8eb-c069b6e9d349",
      "metadata": {
        "id": "d525ecbf-7348-477f-a8eb-c069b6e9d349"
      },
      "source": [
        "## Visualize the Parameters and Gradients of Different Dimensions\n",
        "\n",
        "In this section, we will visualize the gradients and parameters of two gradient descent methods in each iteration of training.\n",
        "\n",
        "### Gradient Descent w/o Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186cdf96-f350-4110-a7fd-b616ea07e0f8",
      "metadata": {
        "id": "186cdf96-f350-4110-a7fd-b616ea07e0f8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,0], 'r', label=\"Dimension 0\")\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,1], 'b', label=\"Dimension 1\")\n",
        "plt.title(\"Gradients\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,0], 'r', label=\"Dimension 0\")\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,1], 'b', label=\"Dimension 1\")\n",
        "plt.title(\"Parameters\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ec155b-3782-434f-97cc-be617dcb6b0d",
      "metadata": {
        "id": "94ec155b-3782-434f-97cc-be617dcb6b0d"
      },
      "source": [
        "### Gradient Descent with Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0933f153-76de-466c-a098-9ec046cdbfaa",
      "metadata": {
        "id": "0933f153-76de-466c-a098-9ec046cdbfaa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,0], 'r', label=\"Dimension 0\")\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,1], 'b', label=\"Dimension 1\")\n",
        "plt.title(\"Gradients\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,0], 'r', label=\"Dimension 0\")\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,1], 'b', label=\"Dimension 1\")\n",
        "plt.title(\"Parameters\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68bde13-460a-462b-8f94-17ca0cde41c6",
      "metadata": {
        "id": "d68bde13-460a-462b-8f94-17ca0cde41c6"
      },
      "source": [
        "**Question: How does $\\sigma_i$ (the eigenvalues) influence the gradients and paramters updates?** Please answer this question in your written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028dba9f-956c-4d77-b2c4-9c76baec1052",
      "metadata": {
        "id": "028dba9f-956c-4d77-b2c4-9c76baec1052"
      },
      "source": [
        "## Compare gradient descent and gradient desent with momentum\n",
        "### Comparing gradient changes with different iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d841787-7a12-4c99-9ec5-2bff041bb6ef",
      "metadata": {
        "id": "5d841787-7a12-4c99-9ec5-2bff041bb6ef"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,0], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,0], 'b', label=\"momentum\")\n",
        "plt.title(\"Gradients of Dimension 0\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,1], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,1], 'b', label=\"momentum\")\n",
        "plt.title(\"Gradients of Dimension 1\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e76fe9-151e-4b02-bd78-d7065d55b96b",
      "metadata": {
        "id": "77e76fe9-151e-4b02-bd78-d7065d55b96b"
      },
      "source": [
        "### Comparing parameter changes with different iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e77003-4328-4a3b-a232-fb6d3f2e8d14",
      "metadata": {
        "id": "d8e77003-4328-4a3b-a232-fb6d3f2e8d14"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,0], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,0], 'b', label=\"momentum\")\n",
        "plt.title(\"Parameters of Dimension 0\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,1], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,1], 'b', label=\"momentum\")\n",
        "plt.title(\"Parameters of Dimension 1\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3252810-248b-4cfb-9c49-db7b3156a25b",
      "metadata": {
        "id": "d3252810-248b-4cfb-9c49-db7b3156a25b"
      },
      "source": [
        "### Comparing loss with different iterations\n",
        "Note that to maximize the visibiity, we will visualize $\\log (\\text{loss}_i-loss_*)$, where $\\text{loss}_i$ is the loss at iteration $i$ and $loss_*$ is the optimal loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2974b998-323a-4534-bfd4-a420d5b83389",
      "metadata": {
        "id": "2974b998-323a-4534-bfd4-a420d5b83389"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.log(np.abs(losses)-losses[-1]), 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.log(np.abs(losses_m)-losses_m[-1]), 'b', label=\"momentum\")\n",
        "plt.title(\"Loss changes as iterations increase\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Log(loss(at iteration $i$) - optimal loss)\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef83e79-4576-4949-ab4a-365c90ca6d4e",
      "metadata": {
        "id": "6ef83e79-4576-4949-ab4a-365c90ca6d4e"
      },
      "source": [
        "**Question 1: Comparing gradient descent and gradient descent with momentums, which one converges faster for this task? Why?** Please answer this question in your written assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jGkfVQlyGPNT",
      "metadata": {
        "id": "jGkfVQlyGPNT"
      },
      "source": [
        "**Question 2: If one method converges faster, can you try change the learning rate to further accelerate the convergence? Please re-run one method with different learning rate, and compare the gradients, parameters and loss with the other method.**\n",
        "*Hint: The learning rate cannot be too large, otherwise the function may not converge.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Dx8AFXhG0mk",
      "metadata": {
        "id": "2Dx8AFXhG0mk"
      },
      "outputs": [],
      "source": [
        "maxiter = 100\n",
        "###############################################\n",
        "###       TODO: YOUR CODE HERE              ###\n",
        "###############################################\n",
        "stepsize = ?\n",
        "beta = ?\n",
        "###############################################\n",
        "###       END OF YOUR CODE                  ###\n",
        "###############################################\n",
        "grads_m, ws_m, losses_m = runGDM(maxiter, stepsize, beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SZSyuep_I1q4",
      "metadata": {
        "id": "SZSyuep_I1q4"
      },
      "source": [
        "### After changing learning rate, compare gradient changes with different iterations\n",
        "*Hint: You should see now that a dimension will have much larger gap between two methods. That is a benefit from larger learning rate.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C0-S2V0BIwNc",
      "metadata": {
        "id": "C0-S2V0BIwNc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,0], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,0], 'b', label=\"momentum\")\n",
        "plt.title(\"Gradients of Dimension 0\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(grads)[:,1], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(grads_m)[:,1], 'b', label=\"momentum\")\n",
        "plt.title(\"Gradients of Dimension 1\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CQ0LKrxJI2gW",
      "metadata": {
        "id": "CQ0LKrxJI2gW"
      },
      "source": [
        "### After changing learning rate, compare parameter changes with different iterations\n",
        "*Hint: You should see now that a dimension will have much larger gap between two methods. That is a benefit from larger learning rate.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMIE_asEIshn",
      "metadata": {
        "id": "jMIE_asEIshn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,0], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,0], 'b', label=\"momentum\")\n",
        "plt.title(\"Parameters of Dimension 0\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.abs(ws)[:,1], 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.abs(ws_m)[:,1], 'b', label=\"momentum\")\n",
        "plt.title(\"Parameters of Dimension 1\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VI951JPKI6CX",
      "metadata": {
        "id": "VI951JPKI6CX"
      },
      "source": [
        "### After changing learning rate, compare loss changes with different iterations\n",
        "*Hint: You should see now one method is much faster than the other one.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MJyvowdaG947",
      "metadata": {
        "id": "MJyvowdaG947"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(range(maxiter), np.log(np.abs(losses)-losses[-1]), 'r', label=\"GD\")\n",
        "plt.plot(range(maxiter), np.log(np.abs(losses_m)-losses_m[-1]), 'b', label=\"momentum\")\n",
        "plt.title(\"Loss changes as iterations increase\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Log(loss(at iteration $i$) - optimal loss)\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}